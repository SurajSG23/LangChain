{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b142d79c-3972-4d49-9e57-23f9175f946b",
   "metadata": {},
   "source": [
    "# LangChain Expression Language\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "* A declarative way to build AI pipelines in LangChain.\n",
    "* Instead of long Python code, you connect components in a chain-like style.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LCEL?\n",
    "\n",
    "* Shorter and cleaner code.\n",
    "* Easier to read and maintain.\n",
    "* Works the same for synchronous, asynchronous, batch, and streaming execution.\n",
    "* Good for debugging and reusing components.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Example\n",
    "\n",
    "Without LCEL:\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
    "result = chain.run(\"cats\")\n",
    "```\n",
    "\n",
    "With LCEL:\n",
    "\n",
    "```python\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "```\n",
    "**prompt, ChatOpenAI(), StrOutputParser() are called runnables**\n",
    "\n",
    "---\n",
    "\n",
    "## LCEL Operators\n",
    "\n",
    "* `|` → Pipe operator, connects steps.\n",
    "* `.invoke()` → Run once with input.\n",
    "* `.batch()` → Run for multiple inputs.\n",
    "* `.stream()` → Get outputs step by step (streaming responses).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. Composable – you can plug and play different parts.\n",
    "2. Parallel and sequential execution supported.\n",
    "3. Unified API – works the same for sync, async, batch, and streaming.\n",
    "4. Production-ready – optimized for speed and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Summary\n",
    "\n",
    "* LCEL is a short and readable way to connect LangChain components.\n",
    "* It works like piping data through different steps.\n",
    "* Use `|` to chain steps.\n",
    "* The same code works for real-time, async, or batch calls.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943da09-e209-47bd-91ee-5dba9306f6ff",
   "metadata": {},
   "source": [
    "## Piping a prompt, model, and an output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bb950f-0685-4b2b-b7ba-2150ef6e40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5ce49c-da69-46b2-8713-9b5d2efc542d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_instructions = CommaSeparatedListOutputParser().get_format_instructions()\n",
    "\n",
    "list_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b17085af-9b8a-4777-8452-b14ef76040a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've recently adopted a {pet}. Could you suggest three {pet} names? \n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "# Chat Template\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('human', \"I've recently adopted a {pet}. Could you suggest three {pet} names? \\n\" + list_instructions)\n",
    "])\n",
    "\n",
    "print(chat_template.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e0401b-e7a5-4669-acbd-140186f1d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model\n",
    "\n",
    "def AskGeminiAI(question):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=config.gemini_api_key,\n",
    "        temperature=0.7,\n",
    "        max_tokens=250,\n",
    "        model_kwargs={\"seed\": 42}\n",
    "    )\n",
    "    response = llm.invoke(question)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adda5920-af39-493f-bf36-9bf98d75bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output parser\n",
    "\n",
    "list_output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e97fcdfe-14bd-4890-b5e6-089517573149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Buddy', 'Luna', 'Charlie']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 step\n",
    "\n",
    "chat_template_result = chat_template.invoke({'pet':'dog'})\n",
    "\n",
    "chat_result = AskGeminiAI(chat_template_result)\n",
    "\n",
    "list_output_parser.invoke(chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "815f422c-cd7b-4f7e-8ba5-a860b1ad2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Buddy', 'Luna', 'Charlie']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single line\n",
    "\n",
    "chain = chat_template | AskGeminiAI | list_output_parser\n",
    "\n",
    "chain.invoke({'pet':'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ae0f8-e7dd-4fcc-a029-de74ff8fcc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba2333-e83d-419a-bb0c-d95d500035b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9088e-e47a-4667-a8f8-3645963bc055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11253f37-b372-4348-bb44-c75491ff1fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf514fb-6b42-425c-8e9e-da3f0131f6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037aa20-0036-46dd-80c6-9d7f0d34ca44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74f5bc-f377-4844-be61-9df1785b0e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f37259-2f5a-4a05-955a-6c670ea2d1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
